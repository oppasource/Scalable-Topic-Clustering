{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5320e393-5696-48a6-ac68-a47a1516a138",
   "metadata": {},
   "source": [
    "# Getting topic for large number of documents (messages)\n",
    "\n",
    "We have huge number of documents (say 1 Billion+ messages). We want to find broad level topics which these documents belong to.\n",
    "\n",
    "This is a demonstration of how it can be done using 10M documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee322db-d675-45c7-b21d-ed00a6dd973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6dce7-e1a4-4104-b826-8adf2c3ac64d",
   "metadata": {},
   "source": [
    "## Generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a64a04-defe-43ab-bf04-573b1448328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have embeddings saved for all messages (in this example we saved embeddings for 10M messages)\n",
    "num_entries = 10000000\n",
    "embedding_dim = 128\n",
    "\n",
    "embeddings = np.random.rand(num_entries, embedding_dim).astype('float32')\n",
    "np.save(\"message_embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85679bf0-bb71-48f8-8bf2-4f8c8bd5547f",
   "metadata": {},
   "source": [
    "## Using synthetic data to train Kmeans - incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c03bc8-c776-4bf1-946b-0fe7097ccfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, batch_size):\n",
    "    \"\"\"\n",
    "    Stream embeddings from a large .npy file in batches.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the stored embeddings file.\n",
    "    - batch_size (int): Number of embeddings to load per batch.\n",
    "    \n",
    "    Yields:\n",
    "    - np.ndarray: Batch of embeddings.\n",
    "    \"\"\"\n",
    "    # Memory-map the large file to avoid loading all at once\n",
    "    embeddings = np.load(file_path, mmap_mode='r')\n",
    "    num_entries = embeddings.shape[0]\n",
    "    \n",
    "    # Yield embeddings in chunks\n",
    "    for start_idx in range(0, num_entries, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_entries)\n",
    "        yield embeddings[start_idx:end_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99be0a32-e1c7-470d-9923-8377c45fbe02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Training clusters incrementally...\n",
      "Processing batch 1\n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.14 s\n",
      "Processing batch 296 s, search 0.78 s): objective=255091 imbalance=1.007 nsplit=0       \n",
      "\n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.82 s\n",
      "Processing batch 385 s, search 0.73 s): objective=255448 imbalance=1.009 nsplit=0       \n",
      "  Iteration 19 (0.91 s, search 0.78 s): objective=255431 imbalance=1.009 nsplit=0       \n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 2.43 s\n",
      "Processing batch 410 s, search 0.94 s): objective=255189 imbalance=1.005 nsplit=0       \n",
      "\n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 2.85 s\n",
      "Processing batch 595 s, search 0.80 s): objective=255347 imbalance=1.007 nsplit=0       \n",
      "  Iteration 19 (1.00 s, search 0.84 s): objective=255329 imbalance=1.007 nsplit=0       \n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.71 s\n",
      "Processing batch 620 s, search 1.03 s): objective=255183 imbalance=1.008 nsplit=0       \n",
      "\n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.28 s\n",
      "Processing batch 708 s, search 0.88 s): objective=255349 imbalance=1.008 nsplit=0       \n",
      "  Iteration 19 (1.19 s, search 0.97 s): objective=255332 imbalance=1.008 nsplit=0       \n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.20 s\n",
      "Processing batch 8  Iteration 19 (1.38 s, search 1.19 s): objective=255190 imbalance=1.007 nsplit=0       \n",
      "\n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.23 s\n",
      "Processing batch 982 s, search 0.69 s): objective=255297 imbalance=1.004 nsplit=0       \n",
      "\n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.46 s\n",
      "Processing batch 101 s, search 0.95 s): objective=255160 imbalance=1.006 nsplit=0       \n",
      "  Iteration 19 (1.16 s, search 0.99 s): objective=255146 imbalance=1.006 nsplit=0       \n",
      "Sampling a subset of 25600 / 1000000 for training\n",
      "Clustering 25600 points in 128D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.21 s\n",
      "  Iteration 19 (1.22 s, search 1.03 s): objective=255212 imbalance=1.011 nsplit=0       \r"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "embedding_dim = 128  # Dimension of embeddings\n",
    "batch_size = 1000000   # Size of each batch\n",
    "n_clusters = 100     # Number of clusters\n",
    "file_path = \"message_embeddings.npy\"\n",
    "\n",
    "# Step 1: Initialize FAISS KMeans\n",
    "kmeans = faiss.Kmeans(d=embedding_dim, k=n_clusters, niter=20, verbose=True)\n",
    "\n",
    "# Step 2: Train KMeans incrementally\n",
    "print(\"Step 2: Training clusters incrementally...\")\n",
    "for batch_idx, embeddings in enumerate(load_embeddings(file_path, batch_size)):\n",
    "    print(f\"Processing batch {batch_idx + 1}\")\n",
    "    kmeans.train(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c259c805-ab43-49fb-b4be-d97ba262ee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.centroids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffc463-0edd-4e4f-9e2c-7d5950c75bc6",
   "metadata": {},
   "source": [
    "## Assign each message embedding to cluster centroid and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87ee69d-44d5-4ea0-98b4-1de832e01122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Assigning clusters to all documents incrementally...\n",
      "Processing batch 1\n",
      "Processing batch 2\n",
      "Processing batch 3\n",
      "Processing batch 4\n",
      "Processing batch 5\n",
      "Processing batch 6\n",
      "Processing batch 7\n",
      "Processing batch 8\n",
      "Processing batch 9\n",
      "Processing batch 10\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create an Index for clustering assignments\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(kmeans.centroids)  # Add cluster centroids to the index\n",
    "\n",
    "# Step 4: Assign clusters to full dataset incrementally\n",
    "cluster_assignments = []\n",
    "\n",
    "print(\"Step 4: Assigning clusters to all documents incrementally...\")\n",
    "for batch_idx, embeddings in enumerate(load_embeddings(file_path, batch_size)):\n",
    "    print(f\"Processing batch {batch_idx + 1}\")\n",
    "    _, batch_assignments = index.search(embeddings, 1)  # Get nearest centroid for each embedding\n",
    "    cluster_assignments.append(batch_assignments)\n",
    "\n",
    "# Combine all assignments into one array\n",
    "cluster_assignments = np.concatenate(cluster_assignments).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d9b5c6a-f6ce-4746-a66f-5e153c578975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents clustered: 10000000\n",
      "Cluster distribution: [100651  97296 107087  94618 107199  94960 109830  97594 104802 109454\n",
      " 106202 104366 106969 101504 101069  92804 104876  99182 100138  95491\n",
      " 102317  87704  99995  56437  99547 101298 103724 111784 104271  93560\n",
      " 102506 104545  98106 107408 100971  99000 105377 101251  80115  98058\n",
      " 101601 103384  99340 104280  94726 102630  98192 105902  96861  89314\n",
      "  99186 101826 106090  92036 104614  93061  98762  97327 101876 101358\n",
      " 102184 102623 103350 104055 107690  97801 106645  99997  99244 105845\n",
      " 104599 106911  99740  93440  92541  92483 100642 106052 100756 108349\n",
      "  92424 103153  98404 103952  97227 102327  98601  93737 100425 101367\n",
      " 107152  98198  96579 101784 103939  96267  97935  98483  87883  98784]\n"
     ]
    }
   ],
   "source": [
    "# Save the cluster assignments to a file (optional)\n",
    "np.save(\"cluster_assignments.npy\", cluster_assignments)\n",
    "\n",
    "# Analyze results\n",
    "print(f\"Number of documents clustered: {len(cluster_assignments)}\")\n",
    "print(f\"Cluster distribution: {np.bincount(cluster_assignments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3f934-7aec-45a2-8e1e-0dd8eb535890",
   "metadata": {},
   "source": [
    "## Next Steps and additional ideas\n",
    "\n",
    "- Take N random sample from each cluster and send it through prompt to get a topic label for that cluster.\n",
    "- May have to experiment a bit with number of clusters. Would be a good idea to keep the number as high as possible, so similar topics can be normalized later and clusters can be merged.\n",
    "- Can go a step further to retrieve most similar using the topic label within that cluster and prompt to different topic for least similar messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af4d55-5a10-410f-9f1b-0a68563e7453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
